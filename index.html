<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Blackboard Usability Study: HCI Assignment 2</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this project -->
  <link href="https://fonts.googleapis.com/css?family=Catamaran:100,200,300,400,500,600,700,800,900" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Lato:100,100i,300,300i,400,400i,700,700i,900,900i" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:100,100i,300,300i,400,400i,700,700i,900,900i" rel="stylesheet">

  <!-- Custom style for this project -->
  <link href="css/style.css" rel="stylesheet">

  <!-- Custom icons for this project-->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz"
    crossorigin="anonymous">

</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-custom fixed-top">
    <div class="container">
      <a class="navbar-brand" href="#"><img class="brand-logo" src="img/blackboard-logo.jpeg" alt="Blackboard Logo"></a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive"
        aria-expanded="false" aria-label="Toggle navigation"><i class="fa-bars fas"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="#introduction">Introduction</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#usabilitytesting">Usability Testing</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#research">Research</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#conclusion">Conclusion</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#team">Team</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#footer">Documents</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>


  <header class="masthead text-center text-white">
    <div class="masthead-content">
      <div class="container">
        <h3 class="masthead-heading mb-0 mt-4">Blackboard Usability Study</h3>
        <a href="#introduction" class="btn btn-primary btn-lg rounded-pill mt-4">Explore</a>
      </div>
    </div>

  </header>
  <div class="col-lg-12 bg-ghostwhite">
    <img class="hero-img" src="img/blackboard-laptop-hero.jpg" alt="Blackboard Website">
  </div>


  <section id="introduction">
    <div class="container section-style">
      <div class="row align-items-center">
        <div class="col-lg-12">
          <div class="p-5 text-center">
            <h1>Introduction</h1>
            <p>Blackboard is a learning management system employed by many tertiary education providers globally. The
              website provides end users with the ability to organise, store, share, and assess academic content As
              this system’s user interface can vary significantly per institute, it is essential that end users
              relevant to the institute in question are the main focus for its implementation.
              End-user experience and interaction are vital in determining the success of any product and Blackboard is
              no exception. By measuring the five Usability quality components, we can identify areas in need of
              improvement to increase its ease of use. The five areas identified are Learnability, Efficiency,
              Memorability, Errors, and Satisfaction (Nielsen, 2012) </p>
            <p>
              As the study seeks to identify the issues experienced by the existing user base, Efficiency,
              Memorability, Errors, and Satisfaction were defined as the main categories of interest. Due to their
              previous exposure to Blackboard, Learnability was unable to be measured. The four identified categories
              are defined by Isa, Lokman, Wahid, & Sulaiman (2014) as follows.
              <ul>
                <li> Efficiency is a measure of the speed with which users can perform tasks. This category seeks to
                  measure
                  the ease with which end users can operate the website, and ensures the minimum amount of user input.</li>
                <li> Memorability measures how easily users returning to the design can perform previously performed
                  tasks.
                  This seeks to assess how impactful the design has been at conveying initial knowledge, in addition to
                  providing a natural flow through the website.</li>
                <li>
                  Errors are the measure of the frequency and severity of end-user mistakes, in addition to their
                  ability
                  to recover from said errors. This metric seeks to measure the intuitiveness of existing users to
                  minimise
                  their frustration or difficulty using the website</li>
                <li>
                  And the measure of user Satisfaction with the design. This seeks to gauge how pleasant interactions
                  with
                  the interface are, and the general feeling users have upon viewing the motif.</li>
              </ul>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="divider-light"></div>

  <section id="usabilitytesting">
    <div class="container section-style">
      <div class="row align-items-center">
        <div class="col-lg-12 text-center">
          <div class="py-5">
            <h1>Usability Testing</h1>
            <div class="py-2">
              <img class="img-fluid section-img" src="img/usability-testing.png" alt="">
            </div>
          </div>
        </div>

        <div class="col-lg-8 mx-auto text-center">
          <span class="small-body-text">The usability testing can be described as the most fundamental usability
            method.
            There are four main types of usability testing: <br /><br />

            - Exploratory, to test preliminary concepts and evaluate their promise.<br /><br />

            - Assessment, to test features during implementation.<br /><br />

            - Comparison, to assess on design against another.<br /><br />

            - Validation, to certify that features meet certain standards and benchmarks late in the development
            process.<br /><br />

            Every usability test has the primary goal of improving the usability
            of the end product and satisfying the needs of a particular group of
            users or the implementation of the product in a particular environment</span>
        </div>
      </div>
    </div>
  </section>

  <div class="divider-light"></div>

  <section id="research">
    <div class="container section-style">
      <div class="row align-items-center">
        <div class="col-lg-12 text-center pt-5">
          <h1>Research Framework<i class="heading-icon fas fa-book"></i></h1>
        </div>
        <div class="py-5">
          <div class="col-lg-12">
            <div class="text-center">
              <img class="img-fluid graph-img" src="img/framework-diagram.png" alt="">
            </div>
            <h2>Pre-Study</h2>
            <p>Pre-study activities include all research processes that take place before the usability experiment.
              Before the usability session, all participants are asked questions oriented around their demographic,
              previous experience with the software, and how long they have been using Blackboard. These questions seek
              to measure how the participants feel about the software leading up to the study, their level of expertise
              with the software, and how their background may influence the results.</p>
            <h2>Experiment</h2>
            <p>The experiment involves participants completing three tasks in a one-to-one usability session. All tasks
              begin from the homepage of a logged-in blackboard user. The three tasks are as follows:
              <ol>
                <li>Navigate to any lecture slide for a designated subject.</li>
                <br />
                <li>Retrieve the grade for a specific assignment.</li>
                <br />
                <li>Find the date and time the requested exam will take place.</li>
              </ol>
              The completion time was independently recorded for each task, starting once the participant was on the
              home page and indicated they were ready to start. Recording was stopping once the set condition for the
              task was met, e.g. reading aloud the grade the for the assignment. After each task was completed, the
              participant was invited to discuss their experience with the session moderator. This discussion was
              taped, following agreement from the participants.
            </p>
            <h2>Post-Study</h2>
            <p>A post-study questionnaire of 10 questions was provided covering the features, user interface, and
              performance of the website after the usability experiment. The survey required users to indicate their
              level of agreement with different statements on a scale of 1 - 5 (1 - Strongly Disagree; 2 - Disagree;
              3 - Neutral; 4 - Agree; 5 - Strongly Agree). The questionnaire incorporates sections that enable
              participants
              to provide qualitative written data in the form of comments and recommendations. </p>
          </div>
        </div>

      </div>
    </div>
  </section>

  <div class="divider-light"></div>

  <section id="results">
    <div class="container section-style">
      <div class="row align-items-center">
        <div class="col-lg-12">
          <div class="py-5">
            <h1 class="mb-5">Results & Analysis<i class="heading-icon fas fa-search"></i></h1>
            <div class="row py-3">
              <span class="small-body-text pb-5 px-3">
                <h2>Pre-Study</h2>
                The participants were all third-year university students aged between 20 and 28. There were 11 males
                and one female. 11 of the students were studying towards a Bachelor of Computer and Information
                Science, and one student was studying towards a Bachelor of Science. Chosen majors included analytics,
                software development, computational intelligence and computer networking. All participants have been
                using the blackboard system for at least three years, and two had been using it for four and five years
                respectively. Majority of participants had no experience with any educational management system before
                using blackboard. Two participants had used the Canvas system at the University Of Auckland, and
                another two had used similar systems in high school. The pre-experiment impression of the Blackboard
                system was mostly positive, and all but one participant believed the application fulfilled its role as
                a tool for education management. Many participants commented that they found it very easy to use and
                that it was beneficial in organising their study content. Some of the negative preliminary feedback
                mentioned that the site had poor smartphone optimisation, can be challenging to learn and needs to be
                more intuitive when it came to finding exams.
              </span>
            </div>

            <div class="row py-3">
              <span class="small-body-text pb-5 px-3">
                <h2>Experiment</h2>
                <b>Method:</b> For each of our three tasks our participants were timed from the moment
                they touched the laptop until the desired outcome was achieved or the participant
                abandoned the task. Each task had a different desired outcome. Task one was when the
                user opened a lecture slide, task two was when the user read the overall grade aloud
                and task three was when the user read the exam time information aloud.
              </span>
              <div class="col-lg-6">
                <span class="small-body-text pt-5">
                  <b>Task One</b><br />
                  Task one was to open a lecture slide within the COMP719 – Applied Human Computer Interaction
                  paper. All users were successfully able to complete this task as most of them had experience
                  with the blackboard system, however some confusion was present due to the pages being named
                  “course notes”, “lecture notes” & “resources” in other papers within blackboard system. <br /><br />

                  From table 1, the following statistics can be mentioned:<br /><br />
                  - Total of Time Completion: <b>260.51 seconds</b><br />
                  - Average Completion Time: <b>21.61 seconds</b>
                </span>
              </div>
              <div class="col-lg-6 text-center">
                <span><i class="tiny-body-text">Table 1: Task 1 Completion Times</i><br /></span>
                <img class="table-img" src="img/Task 1 - Completion Times.png" alt="">
              </div>
            </div>
            <div class="row py-3">
              <div class="col-lg-6">
                <span class="small-body-text pt-5">
                  <b>Task Two</b><br />
                  Task two was to identify the total grade for ENSE701 Contemporary Methods in Software Engineering
                  Assignment One. If the grade was wrong we told the participant it was wrong and asked them to
                  continue
                  looking. All users were successfully able to complete this task, however several users were uncertain
                  whether they had found the correct grade, as the overall grade was not clearly labelled and there
                  were
                  multiple navigation pathways to the grade which added to the confusion.<br /><br />

                  From table 2, the following statistics can be mentioned:<br /><br />
                  - Total of Time Completion: <b>422.28 seconds</b><br />
                  - Average Completion Time: <b>35.19 seconds</b>
                </span>
              </div>
              <div class="col-lg-6 text-center">
                <span><i class="tiny-body-text">Table 2: Task 2 Completion Times</i><br /></span>
                <img class="table-img" src="img/Task 2 - Completion Times.png" alt="">
              </div>
            </div>
            <div class="row py-3">
              <div class="col-lg-6">
                <span class="small-body-text pt-5">
                  <b>Task Three</b><br />
                  Task three was to identify the date and time that the exam for COMP700 Text and Vision Intelligence
                  will take place. Most users noted that the placement of the timetable was unusual and not intuitive,
                  and that without previous experience this would be a difficult task to complete. This was proved when
                  participant 5 did not have any previous experience with the exam timetable and was unable to complete
                  the task. Since participant 5 could not complete the task they were allotted the max time as a
                  default.
                  His result was omitted from the average completion time for consistency but was left in the total
                  time.<br /><br />

                  From table 3, the following statistics can be mentioned:<br /><br />
                  - Total of Time Completion: <b>858.28 seconds</b><br />
                  - Average Completion Time: <b>39.89 seconds</b>
                </span>
              </div>
              <div class="col-lg-6 text-center">
                <span><i class="tiny-body-text">Table 3: Task 3 Completion Times</i><br /></span>
                <img class="table-img" src="img/Task 3 - Completion Times.png" alt="">
              </div>
              <div class="col-lg-12 my-5">
                <p><b>Finding:</b> We ordered the tasks from top to bottom where the first task consisted
                  of something which all students do on a daily or weekly basis (opening the lecture notes),
                  followed by viewing grades (done monthly or more) and lastly viewing exam timetables (done
                  end of the semester). This gradual increase allowed us to judge memorability within the
                  users as we noticed that each task took longer than the one before, where task one averaged
                  at 21.62 seconds, task two averaged at 35.19 seconds and task three averaged at 39.89 seconds
                  (after taking the outliers out). This in turn proves the memorability part of our research
                  criteria where tasks that users have done more often are completed faster. Task one and
                  task three required the same amount of clicks to open and load the file.</p>
              </div>
            </div>
            <div class="col-lg-12 mt-3">
              <div class="p-3 text-center">
                <h4 class="p-2 mb-4">User Feedback - Key Takeaways</h4>
                <img class="img-fluid" src="img/user-task-feedback-key-takeaways.png" alt="">
              </div>
            </div>
            <div class="row py-3">
              <span class="small-body-text pb-5 px-3">
                <h2>Post-Study</h2>
                <p> <b>Method:</b> Once the usability tasks were complete, participants completed a questionnaire.
                  Participants indicated a level of agreement with ten statements, using a 1-5 scale, and had the
                  option
                  to write an additional comment per statement. For analysis, the results of the questionnaire are
                  grouped into three usability categories; features, user interface and performance.</p>
                <h3>Features</h3>
                <div class="center-image-div">
                  <span><i class="tiny-body-text">Table 4: Summary of Participants Score On Features</i><br /></span>
                  <img class="img-fluid" src="img//Agreement Scale.PNG" alt="">
                  <img class="img-fluid" src="img/Feature Points.PNG" alt="">
                  <div><i class="tiny-body-text">Table 5: Summary of Participants Comments On Features</i><br /></div>
                  <img class="img-fluid" src="img/FEATURE Comments.PNG" alt="">
                </div>
                <p>The first four questions of the usability study focussed on features of the website with a focus on
                  interactions with navigation menus and links.</p>
                <p>
                  When looking at features of the website, participant highlighted navigation as a potnetial usability
                  issues issue. Participants noted they need to click through too many menus, and that the pages
                  contained information not relevant to what they are doing. They commented that navigation menus were
                  not always clear where they would take the user. Finding exam timetables was highlighted as
                  particularly difficult and potentially undoable without prior experience using the Blackboard system.
                  At least two participants indicated that finding the exam time table would be impossible if they had
                  not already how to found it previously. However, overall, our participant agreed with positives
                  statements about the features of the website and disagreed with the negative one.
                </p>
                <h3>User Interface</h3>
                <div class="center-image-div">
                  <span><i class="tiny-body-text">Table 6: Summary of Participants Score On User Interface</i><br /></span>
                  <img class="img-fluid" src="img//Agreement Scale.PNG" alt="">
                  <img class="img-fluid" src="img/USER INTERFACE Points.PNG" alt="">
                  <div><i class="tiny-body-text">Table 7: Summary of Participants Comments On User Interface</i><br /></div>
                  <img class="img-fluid" src="img/USER INTERFACE Comments.PNG" alt="">
                </div>
                <p>Questionnaire response shows most participants disagreed with the statement that the user did not
                  like the colour palette. Comments on colour palette were generally favourable to neutral, with only
                  one participant commenting that the colours could be more appealing. Most user’s also agreed that the
                  website was visually appealing, comments included mentioning the colour palette being another
                  critical feature as for why the site was attractive.
                </p>
                <p>
                  On average, there was a marginal agreement with the statement that the user interface was cluttered.
                  This was the only negative statement in the post-questionnaire study that more people agreed with
                  than disagreed. Comments indicated that potentially there are too many things going on per webpage,
                  particularly the home page. It is interesting to note that the average score for users who found
                  Blackboard cluttered (2.5) is very similar to the score of users who found Blackboard hard to
                  navigate (2.75). There could be a possible correlation between the users who find Blackboard easy to
                  see, also end up finding Blackboard easy to navigate. In a study done by Oxford’s Dr David R
                  Danielson (2002) on ‘Web Navigation & Behavioural Effects’, it states that the lack of visual cues
                  present in navigation can contribute to the website feeling “cluttered”, which ties back to our
                  hypothesis and correlation found above.
                </p>
                <h3>Performance</h3>
                <div class="center-image-div">
                  <span><i class="tiny-body-text">Table 8: Summary of Participants Score On Performance</i><br /></span>
                  <img class="img-fluid" src="img//Agreement Scale.PNG" alt="">
                  <img class="img-fluid" src="img/Performance POINTS.PNG" alt="">
                  <div><i class="tiny-body-text">Table 9: Summary of Participants Comments On Performance</i><br /></div>
                  <img class="img-fluid" src="img/PERFORMANCE Comments.PNG" alt="">
                </div>
                <p>Questionnaire results covering the performance of the website scored the two highest positive
                  responses. Participants on average felt load time were appropriate and didn’t agree that the site was
                  unresponsive. Some comments mentioned how fast the website loaded and said it felt responsive as
                  well. One commenter commented that they thought some part of the page could load a bit faster, but
                  otherwise the usability feedback indicate usability issues of performance are very limited,
                  suggesting the website may be well optimised. A contributing factor to the positive performance of
                  the Blackboard site may be the high broadband speed available at the Auckland University of
                  Technology, where the usability studies took place. Testing the perceived performance of the site at
                  differing speeds of broadband would be useful, but is beyond the scope of this project.
                </p>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="divider-light"></div>

  <section id="conclusion">
    <div class="container section-style">
      <div class="row mt-5">
        <div class="col-lg-12 text-center">
          <div class="pb-5">
            <h1>Conclusion & Recommendations</h1>
            <p>Put conclusions here</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="divider-light"></div>

  <section id="team">
    <div class="container section-style">
      <div class="row align-items-center mt-5">
        <div class="col-lg-12 text-center">
          <div class="pb-5">
            <h1>Team & Reflection</h1>
          </div>
        </div>
      </div>
      <div class="row mx-auto">
        <div class="col-lg-6 col-sm-12">
          <div>
            <div class="text-center"><img class="box-shadow-4 img-fluid rounded-circle team-member-img" src="img/dan.jpg"
                alt=""></div>
            <div class="team-member-info">
              <span class="team-member-header">Dan Wood</span><br />
              <span class="team-member-subheader">Role: Usability Tester</span>
            </div>
            <div class="col-lg-12">
              <div class="py-4">
                <h5 class="text-center mb-4"><u>Dan's Reflection</u></h5>
                <span class="tiny-body-text">

                  <b>Degree:&nbsp;&nbsp;</b> Bachelor of Computer and Information Sciences<br />
                  <b>Major:&nbsp;&nbsp;</b> Software Development<br />
                  <b>Year:&nbsp;&nbsp;</b> Three<br /><br />

                  <p style="text-align: justify;">The study I participated in for Applied Human-Computer Interaction
                    (HCI) was a Usability Study
                    of the Blackboard learning management system used at Auckland University of Technology (AUT).
                    I was involved in the research and development section of the study as well as the analysis and
                    organisation of our findings. Once the initial research, experiment, and analysis had been
                    completed I began constructing a webpage to display our findings in an easy to read format.
                    I designed and built the template and assisted in inserting and formatting our results.<br /><br />

                    This study gave me valuable experience of usability testing and the importance of human-centered
                    design. Understanding the issues and difficulties our participants experienced and collecting
                    meaningful data from our experiment gave me insight into where the Blackboard system/website can
                    be improved. I found it interesting that prior experience with Blackboard had a significant impact
                    on usability and navigation with most of our participants mentioning at some point the initial
                    difficulty they faced when using the system for the first time. <br /><br />

                    Specifically, I learned a lot in regard to the Usability Testing Framework and qualitative
                    testing methods such as learnability, efficiency, memorability, errors and satisfaction, and
                    how users interact with a product via these methods. <br /><br />

                    Overall this project was highly beneficial to me as an aspiring User Experience/User Interface
                    designer and will be leaning heavily into all the lessons I learned as I move forward and on to
                    new projects. The big takeaway for me was how important Human Centered Design is in the software
                    or product development cycle - if we can get this part right from the start, the rest of the
                    development will make sense and have a higher chance of success when released to the end users.<br /></p>
                </span>
              </div>
            </div>
          </div>
        </div>
        <div class="col-lg-6 col-sm-12">
          <div>
            <div class="text-center"><img class="box-shadow-4 img-fluid rounded-circle team-member-img" src="img/tristan.jpeg"
                alt=""></div>
            <div class="team-member-info">
              <span class="team-member-header">Tristan Kells</span><br />
              <span class="team-member-subheader">Role: Usability Tester</span>
            </div>
            <div class="col-lg-12">
              <div class="py-4">
                <h5 class="text-center mb-4"><u>Tristan's Reflection</u></h5>
                <span class="tiny-body-text">

                  <b>Degree:&nbsp;&nbsp;</b> Bachelor of Computer and Information Sciences<br />
                  <b>Major:&nbsp;&nbsp;</b> Software Development<br />
                  <b>Year:&nbsp;&nbsp;</b> Three<br /><br />

                  <div style="text-align: justify;">
                    <p>I was responsible for performing some one-on-one usability interviews, researching differing
                      usability methods, contributing questions to the questionnaire & pre-study interview and
                      collecting and analysing the results of the study. As a team, we choose to let every member
                      contribute to the project at all stages, meaning we all got experience with the processes of
                      conducting an academic study.
                    </p>
                    <p> Before this paper, I had absolutely no experience conducting or taking part in any usability
                      studies. As a developer, I often divert my attention to the implementation problems and focus on
                      the things I think would be fun to create, as opposed to thinking about how users will interact
                      with my creation. My work on our year-long R & D project for AUT has highlighted this. Early in
                      the project, we did not include user feedback thoroughly in the development process. Every 4-6
                      weeks we would meet our clients and tell them what we did and how to use our application.
                      Although they would occasionally mention potential usability pitfalls, I feel because we coached
                      and instructed them throughout the processed, we missed out on critical usability data. Once
                      starting Human-Computer Interaction, my team and I started thinking about usability and user
                      experience differently.
                    </p>
                    <p> We didn’t do an official usability study till the last couple weeks of the project. However,
                      the lessons learned were applied more generally far earlier. We began dedicating time at the
                      beginning of each client meeting to having our client interact with the software unguided, and
                      discover new features naturally. This uncovered a lot of usability issues which we had overlooked
                      because of our proximity to the project. For instance, a core feature of our calendar application
                      it the option to specify the office-status of a worker. This feature was activated by a
                      right-click on a particular part of the page. Although we thought this would be self-evident,
                      when showing the application to new users, almost no one was able to discover this feature
                      naturally. Unfortunately, this discovery was made very late into development, so our devised
                      solution lacked usability elegance and took the form of an info pop-up. </p>
                    <p> Conducting this usability study has given me valuable insight into structured methodologies
                      which I believe will help me develop better software going forward.</p>
                  </div>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="row mx-auto">
        <div class="col-lg-6 col-sm-12">
          <div>
            <div class="text-center"><img class="box-shadow-4 img-fluid rounded-circle team-member-img" src="img/saksham.jpeg"
                alt=""></div>
            <div class="team-member-info">
              <span class="team-member-header">Saksham Anand</span><br />
              <span class="team-member-subheader">Role: Usability Tester</span>
            </div>
            <div class="col-lg-12">
              <div class="py-4">
                <h5 class="text-center mb-4"><u>Saksham's Reflection</u></h5>
                <span class="tiny-body-text">

                  <b>Degree:&nbsp;&nbsp;</b> Bachelor of Computer and Information Sciences<br />
                  <b>Major:&nbsp;&nbsp;</b> Computational Intelligence<br />
                  <b>Year:&nbsp;&nbsp;</b> Three<br /><br />

                  <p style="text-align: justify">My name is Saksham Anand, and I am in final semester of Bachelor of
                    Computer & Information
                    Sciences,
                    majoring in Computational Intelligence. Our study was essentially based around the usability of
                    AUT’s
                    blackboard system, with different user demographics. The participants were bought into the lab and
                    given
                    a set of instructions to complete a task, this was combined with pre and post task questionnaire
                    and
                    allowed
                    us to judge the users thought of chain. My role in this project was to work with the team in
                    preparing the
                    pre and post task questionnaires that the user will be using, and writing the research section of
                    the
                    report.</p>

                  <br />
                  <p style="text-align: justify">I discovered some interesting points about the human psychology during
                    the user interviews of
                    the blackboard.
                    In specific, I found that people often like to categorize things in different ways, for example
                    during our
                    experiment some people expected grades to be under the ‘paper page’ on blackboard, however after
                    not
                    finding
                    the grades under paper page, the user then decided to go look for a page which contains all the
                    grades. This
                    was interesting for me because the grades page is technically categorized as well, so I expected
                    users to
                    straight look there. One question this has me wondering is why do categorisations differ between
                    users, for
                    example some people sort out blocks by their shapes, however others may sort out blocks by their
                    colour.</p>

                  <br />I also found the following points interesting about this study:<br /><br />
                  <ul>

                    <li style="text-align: justify">This study gave me insights into User-Centered Design (UCD), as
                      part of the research I discovered the
                      UCD
                      methodology where users are interviewed during the development stages of a software or a website,
                      and
                      based
                      off the interviews the pain points are fixed during development stage. If this methodology is
                      applied
                      to
                      blackboard upgrade development, it would allow the system to be user friendly where it is built
                      on
                      user feedback.</li><br />

                    <li style="text-align: justify">I learnt how people from different backgrounds and diversity have
                      different experience with
                      blackboard, for
                      example some of the mature/older students took a bit longer to navigate blackboard, compared to
                      those
                      who might
                      have used similar knowledge management system in high school (such as Knowledgenet).</li><br />

                    <li style="text-align: justify">Most importantly, I discovered that building softwares and
                      applications is not only about code and
                      project
                      management, its more about the users who will be using the product. There is no point in me
                      spending
                      months
                      coding a software, if the end users don’t understand and like the final outcome.</li>
                  </ul>
                  <br /><br />
                  <p style="text-align: justify">To sum it up, I believe the project accomplished its goal of obtaining
                    insights into area
                    of problems within
                    AUT’s blackboard website. We discovered several points of interest where blackboard could be
                    improved, such as
                    having lecture powerpoints open within browser instead of forcing users to download them. With
                    proper
                    planning
                    for pre and post activity questionnaire we were able to capture and analyze user data from the
                    tasks,
                    which
                    allowed us to check if any correlation existed between users finding website easy to navigate and
                    how
                    fast
                    they completed the task.</p>


                  <br />
                </span>
              </div>
            </div>
          </div>
        </div>
        <div class="col-lg-6 col-sm-12">
          <div>
            <div class="text-center"><img class="box-shadow-4 img-fluid rounded-circle team-member-img" src="img/dylan.jpeg"
                alt=""></div>
            <div class="team-member-info">
              <span class="team-member-header">Dylan Tindale</span><br />
              <span class="team-member-subheader">Role: Usability Tester</span>
            </div>
            <div class="col-lg-12">
              <div class="py-4">
                <h5 class="text-center mb-4"><u>Dylan's Reflection</u></h5>
                <span class="tiny-body-text">

                  <b>Degree:&nbsp;&nbsp;</b> Bachelor of Computer and Information Sciences<br />
                  <b>Major:&nbsp;&nbsp;</b> Software Development<br />
                  <b>Year:&nbsp;&nbsp;</b> Three<br /><br />

                  <p style="text-align: justify;">The study I participated in for Applied Human-Computer Interaction
                    (HCI) was a Usability Study
                    of the Blackboard learning management system used at Auckland University of Technology (AUT).
                    I was involved in the research and development section of the study as well as the analysis and
                    organisation of our findings. Once the initial research, experiment, and analysis had been
                    completed I began constructing a webpage to display our findings in an easy to read format.
                    I designed and built the template and assisted in inserting and formatting our results.<br /><br />

                    This study gave me valuable experience of usability testing and the importance of human-centered
                    design. Understanding the issues and difficulties our participants experienced and collecting
                    meaningful data from our experiment gave me insight into where the Blackboard system/website can
                    be improved. I found it interesting that prior experience with Blackboard had a significant impact
                    on usability and navigation with most of our participants mentioning at some point the initial
                    difficulty they faced when using the system for the first time. <br /><br />

                    Specifically, I learned a lot in regard to the Usability Testing Framework and qualitative
                    testing methods such as learnability, efficiency, memorability, errors and satisfaction, and
                    how users interact with a product via these methods. <br /><br />

                    Overall this project was highly beneficial to me as an aspiring User Experience/User Interface
                    designer and will be leaning heavily into all the lessons I learned as I move forward and on to
                    new projects. The big takeaway for me was how important Human Centered Design is in the software
                    or product development cycle - if we can get this part right from the start, the rest of the
                    development will make sense and have a higher chance of success when released to the end users.<br /></p>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>

  <div class="divider-light"></div>

  <!-- Footer -->
  <footer class="pt-5 mt-5 bg-black" id="footer">
    <div class="container text-center">
      <span><b>Download:</b> &nbsp;</span>
      <a href="./resources/consent-form.pdf"><button class="btn-dark">Consent Form<i class="px-3 fas fa-file-pdf"></i></button></a>
      <a href="./resources/peer-grade.pdf"><button class="btn-dark">Peer Grading<i class="px-3 fas fa-file-pdf"></i></button></a>
    </div>
    <!-- /.container -->
  </footer>

  <div class="divider-light"></div>

  <div class="container py-5">
    <h5><u>References</u></h5>
    <div class="pt-3">
      <span class="tiny-body-text">
        <ul style="line-height: 25px;">
          <li>
            Anderson, J. R. (1991). The Adaptive Nature of Human Categorization - PCL. Retrieved October 18, 2018, from
            http://cognitrn.psych.indiana.edu/rgoldsto/courses/concepts/anderson1991.pdf
          </li>
          <li>
            Danielson, D. R. (2002). Web navigation and the behavioural effects of constantly visible site maps.<i>
              Interacting with Computers, 14</i>(5), 601–618.
          </li>
          <li>
            Holbrook, A. L. (2013). Agree-disagree questions: Problems and some solutions. Retrieved October 5, 2018,
            from http://www.srl.uic.edu/seminars/SRL_Webinar_041114.pdf
          </li>
          <li>
            Isa, W., Lokman, A. M., Wahid, E. S., & Sulaiman, R. (2014). Usability testing research framework: Case of
            Handicraft Web-Based System. <i>2014 2nd International Conference on Information and Communication
              Technology (ICoICT)</i>. doi:10.1109/icoict.2014.6914065
          </li>
          <li>
            Nielsen, J. (2012, January 4). Usability 101: Introduction to Usability. Retrieved October 10, 2018, from
            https://www.nngroup.com/articles/usability-101-introduction-to-usability/
          </li>
          <li>
            Rohrer, C. (2014, October 12), “When to Use Which User Experience Research Methods.”
            Alertbox. Retrieved October 10 , 2018, from
            https://www.xdstrategy.com/wp-content/uploads/2018/08/When-to-Use-Which-User-Experience-Research-Methods-2014-10-12-Print.pdf
          </li>
        </ul>
      </span>
    </div>
  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>